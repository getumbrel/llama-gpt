version: '3.6'

services:
  llama-gpt-api:
    build:
      context: ./api
      dockerfile: Dockerfile
    restart: on-failure
    volumes:
      - ./models:/models
    environment:
      MODEL: '/models/nous-hermes-llama2-13b.ggmlv3.q2_K.bin'
      GPU_LAYERS: 45
      USE_MLOCK: 1
    cap_add:
      - IPC_LOCK
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  llama-gpt-ui:
    image: 'ghcr.io/getumbrel/llama-gpt-ui:latest'
    ports:
      - 3000:3000
    restart: on-failure
    environment:
      - 'OPENAI_API_KEY=sk-XXXXXXXXXXXXXXXXXXXX'
      - 'OPENAI_API_HOST=http://llama-gpt-api:8000'
      - 'DEFAULT_MODEL=/models/llama-2-13b-chat.bin'
      - 'WAIT_HOSTS=llama-gpt-api:8000'
      - 'WAIT_TIMEOUT=600'
